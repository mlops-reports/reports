{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment.api import mlflow as mlflow_api\n",
    "from experiment.utils import transformation\n",
    "\n",
    "import pandas as pd\n",
    "# import pathlib\n",
    "# import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_MODEL_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
     ]
    }
   ],
   "source": [
    "mlflow = mlflow_api.MLFlow()\n",
    "\n",
    "# clean the environment without garbage collection\n",
    "mlflow.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-30 12:21:58 +0300] [92757] [INFO] Starting gunicorn 21.2.0\n",
      "[2023-08-30 12:21:58 +0300] [92757] [INFO] Listening at: http://0.0.0.0:9999 (92757)\n",
      "[2023-08-30 12:21:58 +0300] [92757] [INFO] Using worker: sync\n",
      "[2023-08-30 12:21:58 +0300] [92758] [INFO] Booting worker with pid: 92758\n",
      "[2023-08-30 12:21:58 +0300] [92759] [INFO] Booting worker with pid: 92759\n",
      "[2023-08-30 12:21:58 +0300] [92760] [INFO] Booting worker with pid: 92760\n",
      "[2023-08-30 12:21:58 +0300] [92761] [INFO] Booting worker with pid: 92761\n"
     ]
    }
   ],
   "source": [
    "# run the tracking server in background\n",
    "mlflow.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1 occurs 15 times in the list.\n",
      "Item 3 occurs 24 times in the list.\n",
      "Item 0 occurs 6 times in the list.\n",
      "Item 2 occurs 8 times in the list.\n"
     ]
    }
   ],
   "source": [
    "# item_counts = collections.Counter(clean_annotations[\"classifications\"].to_list())\n",
    "\n",
    "# # 1. Emergency\n",
    "# # 2. Normal\n",
    "# # 3. Non Emergency [Doctor]\n",
    "# # 4. Non Emergency [No Doctor]\n",
    "# for item, count in item_counts.items():\n",
    "#     print(f\"Item {item} occurs {count} times in the list.\")\n",
    "\n",
    "# # manual test the word lemmatizer\n",
    "# import simplemma\n",
    "# word = \"hemisferde\"\n",
    "# simplemma.lemmatize(word, lang=\"tr\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2629\n",
      "Epoch [2/10], Loss: 0.1927\n",
      "Epoch [3/10], Loss: 0.1876\n",
      "Epoch [4/10], Loss: 0.0925\n",
      "Epoch [5/10], Loss: 0.0792\n",
      "Epoch [6/10], Loss: 0.4371\n",
      "Epoch [7/10], Loss: 0.0200\n",
      "Epoch [8/10], Loss: 0.1503\n",
      "Epoch [9/10], Loss: 0.1140\n",
      "Epoch [10/10], Loss: 0.0503\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "\n",
    "# Define a simple feedforward neural network using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 128),  # Input layer (28x28=784 input features, 128 hidden units)\n",
    "    nn.ReLU(),                # ReLU activation function\n",
    "    nn.Linear(128, 64),       # Hidden layer (128 input features, 64 hidden units)\n",
    "    nn.ReLU(),                # ReLU activation function\n",
    "    nn.Linear(64, 10)         # Output layer (64 input features, 10 output units for 10 classes)\n",
    ")\n",
    "\n",
    "# Load MNIST dataset and apply transformations\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28 * 28)  # Reshape images to flatten them\n",
    "        optimizer.zero_grad()               # Zero the gradients\n",
    "        outputs = model(images)            # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()                    # Backpropagation\n",
    "        optimizer.step()                   # Update weights\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 97.18%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test dataset: {accuracy:.2f}%')\n",
    "\n",
    "# Set model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"dataset\": \"report_dataset\",\n",
    "    \"timepoint\": None,\n",
    "    \"n_epochs\": 100,\n",
    "    \"n_folds\": 5,\n",
    "    \"batch_size\": 1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"patience\": None,\n",
    "    \"validation_period\": 5,\n",
    "    \"loss_weight\": 1.0,\n",
    "    \"loss_name\": \"cross_entropy\",\n",
    "    \"layer_sizes\": [\n",
    "        8,\n",
    "        16\n",
    "    ],\n",
    "    \"model_name\": \"default_model_name\",\n",
    "    \"model_save_path\": \"/home/oytun/GitRepos/reports/models/models/default_model_name\",\n",
    "    \"model_params_save_path\": \"/home/oytun/GitRepos/reports/models/models/default_model_name_params.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-30 12:24:29 +0300] [92759] [INFO] Worker exiting (pid: 92759)\n",
      "[2023-08-30 12:24:29 +0300] [92760] [INFO] Worker exiting (pid: 92760)\n",
      "[2023-08-30 12:24:29 +0300] [92757] [INFO] Handling signal: term\n",
      "[2023-08-30 12:24:29 +0300] [92758] [INFO] Worker exiting (pid: 92758)\n",
      "[2023-08-30 12:24:29 +0300] [92761] [INFO] Worker exiting (pid: 92761)\n",
      "[2023-08-30 12:24:29 +0300] [92757] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "if LOG_MODEL_RUN:\n",
    "    # log a model run\n",
    "    log_dict = {\n",
    "        \"params\": model_config,\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"loss\": loss.item()\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # extra_artifacts = {\n",
    "    #     \"tokenizer\": {\n",
    "    #         \"local_path\": tokenizer_path,\n",
    "    #         \"save_path\": \"data\"\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    "    run_id = mlflow.log_experiment_run(\n",
    "        model=model,\n",
    "        experiment_name=\"NLP Experiments\",\n",
    "        run_name=f\"RNN: first_run\",\n",
    "        log_dict=log_dict,\n",
    "        registered_model_name=\"rnn_experiments\",\n",
    "        # extra_artifacts=extra_artifacts,\n",
    "        tags={\"model\": \"deep_learning\"},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
